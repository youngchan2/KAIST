{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnyxHUR1OrZC"
      },
      "source": [
        "In the second part of this assignment, we will implement Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pe0wHTgO34O"
      },
      "source": [
        "### **Preparation**\n",
        "The following code is the preparation for importing packages. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhsR8wuxWNDh"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install imageio\n",
        "!pip install pygame\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9r-GLmKDO3qN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# To visualize\n",
        "import pygame\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import IPython\n",
        "import os\n",
        "\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXBRoGbovCcZ"
      },
      "source": [
        "Before start, we need to upload some assets for visualization.\n",
        "\n",
        "Assets (just for reference; you don't need to download the assets individually):\n",
        "- Elf and stool from https://franuka.itch.io/rpg-snow-tileset\n",
        "- Rock from https://toppng.com/show_download/226268/rock-rock-pixel-art/large\n",
        "- All other assets by Mel Tillery http://www.cyaneus.com/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dJi4vMhvb39"
      },
      "outputs": [],
      "source": [
        "# Upload img.zip\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGojE_SnvonU"
      },
      "outputs": [],
      "source": [
        "!unzip img.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRMT61msO-Bk"
      },
      "source": [
        "### **Q-Learning Implementation**\n",
        "\n",
        "In this assignment, you should use only NumPy to build the Q-learning model.\n",
        "\n",
        "**DO NOT** use other libraries which directly implement Q-learning.\n",
        "\n",
        "**DO NOT** modify other parts of the skeleton code.\n",
        "\n",
        "Follow the comments. They'll give you instructions on what to code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9S7vQapPPsA"
      },
      "source": [
        "### **Step 1. Environment Settings**\n",
        "Here, we give the details of our experimental settings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5CLlw-Gb-iy"
      },
      "source": [
        "#### **Helper function**\n",
        "We provide a helper function to visualize the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpz3Hs0fb94D"
      },
      "outputs": [],
      "source": [
        "# Set parameters for visualization\n",
        "window_size = (512, 512)\n",
        "\n",
        "# Load images\n",
        "hole_image = pygame.image.load(\"img/cracked_hole.png\")\n",
        "rock_image = pygame.image.load(\"img/rock.png\")\n",
        "ice_image = pygame.image.load(\"img/ice.png\")\n",
        "goal_image = pygame.image.load(\"img/goal.png\")\n",
        "start_image = pygame.image.load(\"img/stool.png\")\n",
        "elfs = [\n",
        "    \"img/elf_left.png\",\n",
        "    \"img/elf_down.png\",\n",
        "    \"img/elf_right.png\",\n",
        "    \"img/elf_up.png\",\n",
        "]\n",
        "elf_images = [pygame.image.load(f_name) for f_name in elfs]\n",
        "\n",
        "# Set display\n",
        "pygame.init()\n",
        "pygame.display.init()\n",
        "pygame.display.set_caption(\"SlipperyFrozenLake\")\n",
        "\n",
        "window_surface = pygame.Surface(window_size)\n",
        "cell_width = 64\n",
        "cell_height = 64\n",
        "smaller_cell_scale = 1\n",
        "small_cell_width = int(cell_width * smaller_cell_scale)\n",
        "small_cell_height = int(cell_height * smaller_cell_scale)\n",
        "\n",
        "def _center_small_rect(big_rect, small_dims):\n",
        "    offset_w = (big_rect[2] - small_dims[0]) / 2\n",
        "    offset_h = (big_rect[3] - small_dims[1]) / 2\n",
        "    return (\n",
        "        big_rect[0] + offset_w,\n",
        "        big_rect[1] + offset_h,\n",
        "    )\n",
        "\n",
        "def render(lake, row, col, a_prev):\n",
        "    # Prepare images\n",
        "    #elf_img = elf_images[a_prev]\n",
        "    elf_img = pygame.transform.scale(elf_images[a_prev], (cell_width, cell_height))\n",
        "    hole_img = pygame.transform.scale(hole_image, (cell_width, cell_height))\n",
        "    rock_img = pygame.transform.scale(rock_image, (cell_width, cell_height))\n",
        "    ice_img = pygame.transform.scale(ice_image, (cell_width, cell_height))\n",
        "    goal_img = pygame.transform.scale(goal_image, (cell_width, cell_height))\n",
        "    start_img = pygame.transform.scale(start_image, (small_cell_width, small_cell_height))\n",
        "\n",
        "    for y in range(8):\n",
        "        for x in range(8):\n",
        "            rect = (x * cell_width, y * cell_height, cell_width, cell_height)\n",
        "            if lake[y][x] == \"H\":\n",
        "                window_surface.blit(hole_img, (rect[0], rect[1]))\n",
        "            elif lake[y][x] == \"R\":\n",
        "                window_surface.blit(rock_img, (rect[0], rect[1]))\n",
        "            elif lake[y][x] == \"G\":\n",
        "                window_surface.blit(ice_img, (rect[0], rect[1]))\n",
        "                goal_rect = _center_small_rect(rect, goal_img.get_size())\n",
        "                window_surface.blit(goal_img, goal_rect)\n",
        "            elif lake[y][x] == \"S\":\n",
        "                window_surface.blit(ice_img, (rect[0], rect[1]))\n",
        "                stool_rect = _center_small_rect(rect, start_img.get_size())\n",
        "                window_surface.blit(start_img, stool_rect)\n",
        "            else:\n",
        "                window_surface.blit(ice_img, (rect[0], rect[1]))\n",
        "\n",
        "            pygame.draw.rect(window_surface, (180, 200, 230), rect, 1)\n",
        "    \n",
        "    cell_rect = (\n",
        "        col * cell_width,\n",
        "        row * cell_height,\n",
        "        cell_width,\n",
        "        cell_height,\n",
        "    )\n",
        "\n",
        "    elf_rect = _center_small_rect(cell_rect, elf_img.get_size())\n",
        "    window_surface.blit(elf_img, elf_rect)\n",
        "    return np.transpose(np.array(pygame.surfarray.pixels3d(window_surface)), axes=(1, 0, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOppgAogRLw0"
      },
      "source": [
        "#### **Environment**\n",
        "We will implement a Q-learning framework to train the agent on **SlipperyFrozenLake**, which is modified from the **FrozenLake** provided in `gym` library. The rules are provided as follows:\n",
        "\n",
        "- The agent starts in the starting point (S), and the goal is to reach the goal (G) while avoiding the holes (H).\n",
        "- The agent can choose four actions: Left (0), Down (1), Right (2), Up (3)\n",
        "- If the agent choose one action, the agent moves toward the selected direction until one of the condition is satisfied.\n",
        "1. The agent stops if the rock (R) blocks the agent.\n",
        "2. The agent stops if the agent reaches the boundary of the lake.\n",
        "3. The agent stops if the agent meets the hole (H). This is the case that we should avoid.\n",
        "4. The agent stops if the agent reaches the goal (G). This is the case that we want."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFw_LkVTVS01"
      },
      "source": [
        "We use the following map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yF8xHJ6QOrIJ"
      },
      "outputs": [],
      "source": [
        "### DO NOT MODIFY ###\n",
        "lake = [\n",
        "    \"S-----R-\",\n",
        "    \"-H-R---R\",\n",
        "    \"R----R--\",\n",
        "    \"-HR-RH--\",\n",
        "    \"R----R-R\",\n",
        "    \"-R------\",\n",
        "    \"-H------\",\n",
        "    \"-R-HG-R-\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V7FwXWsVsL8"
      },
      "source": [
        "The set of actions are given as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0DpR5SMOrEu"
      },
      "outputs": [],
      "source": [
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pmzrv_N4cUJi"
      },
      "source": [
        "Using the helper function above (`render`), we can visualize the environment as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGorEiRMcaPT"
      },
      "outputs": [],
      "source": [
        "rgb_array = render(lake, 0, 0, DOWN)\n",
        "img = Image.fromarray(rgb_array)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk9fIsOzc3jW"
      },
      "source": [
        "We implement `next_step`, which calculates (next state, reward, terminate) tuple for a given (state, action) pair.\n",
        "\n",
        "The reward is given only if the agent reaches the goal (+10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7g9jxPxOq7B"
      },
      "outputs": [],
      "source": [
        "def next_step(row, col, action):\n",
        "    reward = 0 # Calculated reward\n",
        "    terminate = False # Boolean value indicating whether the episode is terminated or not\n",
        "\n",
        "    while True:\n",
        "        # 1-1. Implement the case when the agent meets a hole (which terminates the episode).\n",
        "        # At the end, the agent must stand on the hole.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        if lake[row][col] == \"H\":\n",
        "            terminate = True\n",
        "            break\n",
        "        ##############################################################################\n",
        "\n",
        "        # 1-2. Implement the case when the agent meets the goal (which terminates the episode with positive reward of 10).\n",
        "        # At the end, the agent must stand on the goal.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        if lake[row][col] == \"G\":\n",
        "            terminate = True\n",
        "            reward = 10.0\n",
        "            break\n",
        "        ##############################################################################\n",
        "\n",
        "        # 1-3. Implement the case when the agent faces a rock.\n",
        "        # Note that the agent cannot reach to the cell containing rock.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        if lake[row][col] == \"R\":\n",
        "            if action == LEFT:\n",
        "                col += 1\n",
        "            elif action == RIGHT:\n",
        "                col -= 1\n",
        "            elif action == UP:\n",
        "                row += 1\n",
        "            elif action == DOWN:\n",
        "                row -= 1\n",
        "            break\n",
        "        ##############################################################################\n",
        "\n",
        "        # 1-4. Implement the case when the agent reaches the boundary of the lake.\n",
        "        # Note that the agent cannot move out of the boundary.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        if action == LEFT and col == 0:\n",
        "            break\n",
        "        elif action == RIGHT and col == 7:\n",
        "            break\n",
        "        elif action == UP and row == 0:\n",
        "            break\n",
        "        elif action == DOWN and row == 7:\n",
        "            break\n",
        "        ##############################################################################\n",
        "\n",
        "        # 1-5. If no condition is satisfied, prepare next iteration (change the position of the agent)\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        if action == LEFT:\n",
        "            col -= 1\n",
        "        elif action == RIGHT:\n",
        "            col += 1\n",
        "        elif action == UP:\n",
        "            row -= 1\n",
        "        elif action == DOWN:\n",
        "            row += 1\n",
        "        ##############################################################################\n",
        "\n",
        "    return (row, col), reward, terminate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdccaZXcxzTB"
      },
      "source": [
        "We give some testcases to check the implementation of `next_step`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcksON7Ax8vp"
      },
      "outputs": [],
      "source": [
        "assert next_step(0, 0, RIGHT) == ((0, 5), 0, False)\n",
        "assert next_step(0, 0, UP) == ((0, 0), 0, False)\n",
        "assert next_step(3, 7, LEFT) == ((3, 5), 0, True)\n",
        "assert next_step(6, 4, DOWN) == ((7, 4), 10, True)\n",
        "print(\"Sanity check done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePAW0brhlH5l"
      },
      "source": [
        "### **Step 2. Initialize Q-Table**\n",
        "To perform Q-learning, we first need to initialize the Q-table. Q-table contains the state-action values for each (state, action) pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQMmpUbjOq4s"
      },
      "outputs": [],
      "source": [
        "# 2. Initialize Q-table.\n",
        "# Q-table contains the state-action values for each (state, action) pair\n",
        "############################### IMPLEMENT HERE ###############################\n",
        "q_table = np.zeros((64, 4))\n",
        "##############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjYcQnDPu96F"
      },
      "source": [
        "### **Step 3. Implement Epsilon Greedy Strategy**\n",
        "Epsilon greedy strategy is a policy that handles the exploration/exploitation trade-off. For a given epsilon, the agent chooses an action based on the following strategy:\n",
        "- With probability epsilon, we randomly select the possible actions.\n",
        "- With probability 1 - epsilon, we greedily choose the next action based on the Q-table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0fNTnt3vykM"
      },
      "outputs": [],
      "source": [
        "# to_s converts each (row, col) pair into an integer (row * 8 + col), which is the index representing the state (row, col)\n",
        "def to_s(row, col):\n",
        "    return row * 8 + col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htk0IqrnOq2o"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy(row, col, q_table, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        # 3-1. Implement exploration.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        a = np.random.randint(4)\n",
        "        ##############################################################################\n",
        "    else:\n",
        "        # 3-2. Implement exploitation.\n",
        "        # Note that if there are multiple candidates (due to same values), then the agent must randomly choose the action among them.\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        s = to_s(row, col)\n",
        "        a_candidates = np.where(q_table[s] == q_table[s].max())[0]\n",
        "        a = np.random.choice(a_candidates)\n",
        "        ##############################################################################\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUtVprMGxMGY"
      },
      "source": [
        "### **Step 4. Train the agent**\n",
        "In this step, we are going to train the agent.\n",
        "\n",
        "Before training, we first watch the behavior of the (untrained) agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzoMV6yIxhPH"
      },
      "outputs": [],
      "source": [
        "# Generates gif file of the agent's movement based on the input q_table\n",
        "def movement_gif(file, q_table):\n",
        "    row = 0\n",
        "    col = 0\n",
        "    done = False\n",
        "    images = []\n",
        "\n",
        "    rgb_array = render(lake, 0, 0, DOWN)\n",
        "    img = Image.fromarray(rgb_array)\n",
        "    for _ in range(4):\n",
        "        images.append(img)\n",
        "\n",
        "    while not done:\n",
        "        a = epsilon_greedy(row, col, q_table, 0)\n",
        "        \n",
        "        s = to_s(row, col)\n",
        "        (row, col), r, done = next_step(row, col, a)\n",
        "\n",
        "        rgb_array = render(lake, row, col, a)\n",
        "        img = Image.fromarray(rgb_array)\n",
        "        images.append(img)\n",
        "\n",
        "    for _ in range(7):\n",
        "        images.append(img)\n",
        "\n",
        "    imageio.mimsave(file, [np.array(img) for i, img in enumerate(images)], fps=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxvlJFRC1CvT"
      },
      "outputs": [],
      "source": [
        "### DO NOT CHANGE THE SEED ###\n",
        "np.random.seed(123)\n",
        "movement_gif(\"lake_untrained.gif\", q_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EENflUiuN1oV"
      },
      "outputs": [],
      "source": [
        "IPython.display.Image(filename=\"lake_untrained.gif\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lFKtkjrxZ8Y"
      },
      "source": [
        "As we can see, the agent fails to reach the goal. We will train the agent so that the agent can safely reach the goal.\n",
        "\n",
        "First, we set some parameters (learning rate and discount rate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yoauWwo1g_X"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "discount_rate = 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LFSw9KlxZuZ"
      },
      "source": [
        "Then we implement the overall training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFHvsWL4Oq0U"
      },
      "outputs": [],
      "source": [
        "def train_episode(epsilon):\n",
        "    row = 0\n",
        "    col = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 4-1. Choose the action using epsilon greedy algorithm\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        a = epsilon_greedy(row, col, q_table, epsilon)\n",
        "        ##############################################################################\n",
        "\n",
        "        # 4-2. Calculate the next state and the reward\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        (row_new, col_new), r, done = next_step(row, col, a)\n",
        "        ##############################################################################\n",
        "        \n",
        "        # 4-3. Update Q-table\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        s = to_s(row, col)\n",
        "        s_next = to_s(row_new, col_new)\n",
        "        value_old = q_table[s, a]\n",
        "        value_learned = r + discount_rate * np.max(q_table[s_next])\n",
        "        q_table[s, a] = (1 - learning_rate) * value_old + learning_rate * value_learned\n",
        "        ##############################################################################\n",
        "        \n",
        "        # 4-4. Update current state \n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        row = row_new\n",
        "        col = col_new\n",
        "        ##############################################################################\n",
        "\n",
        "def train(num_epoch=10000):\n",
        "    # 4-5. Train the agent.\n",
        "    # We use epsilon = 1 / (i + 1) in i-th episode. (Note that i=0 in the first episode)\n",
        "    ############################### IMPLEMENT HERE ###############################\n",
        "    for i in tqdm(range(num_epoch)):\n",
        "        train_episode(1 / (i + 1))\n",
        "    ##############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_ZW6cSbYjzu"
      },
      "source": [
        "Using the implemented functions, we train the agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ABnzHzYOqyO"
      },
      "outputs": [],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kXJeV2faPJ8"
      },
      "source": [
        "Here, we show the trained Q-table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sub7axz04B0T"
      },
      "outputs": [],
      "source": [
        "q_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcyWCxYCbXPd"
      },
      "source": [
        "### **Step 5. Test the agent**\n",
        "In this step, we will test the trained agent.\n",
        "\n",
        "First, we calculate the average reward for 10000 episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ts2h0kXhbnOc"
      },
      "outputs": [],
      "source": [
        "def test_episode():\n",
        "    row = 0\n",
        "    col = 0\n",
        "    reward_total = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 5-1. Choose the action using greedy algorithm (Note: This step is different from epsilon greedy!)\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        a = epsilon_greedy(row, col, q_table, 0)\n",
        "        ##############################################################################\n",
        "\n",
        "        # 5-2. Calculate the next state and the reward\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        (row_new, col_new), r, done = next_step(row, col, a)\n",
        "        ##############################################################################\n",
        "        \n",
        "        # 5-3. Update current state and total reward\n",
        "        ############################### IMPLEMENT HERE ###############################\n",
        "        row = row_new\n",
        "        col = col_new\n",
        "        reward_total += r\n",
        "        ##############################################################################\n",
        "    \n",
        "    return reward_total\n",
        "\n",
        "def test(num_epoch=10000):\n",
        "    list_reward = []\n",
        "    # 5-4. Test the agent for num_epoch episodes.\n",
        "    # We should save the resulting reward in list_reward.\n",
        "    ############################### IMPLEMENT HERE ###############################\n",
        "    for i in tqdm(range(num_epoch)):\n",
        "        reward = test_episode()\n",
        "        list_reward.append(reward)\n",
        "    ##############################################################################\n",
        "\n",
        "    # 5-5. Calculate the average reward.\n",
        "    ############################### IMPLEMENT HERE ###############################\n",
        "    reward_average = np.mean(list_reward)\n",
        "    ##############################################################################\n",
        "\n",
        "    return reward_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge5Dx1zHcmV9"
      },
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK2H5NeWaY5y"
      },
      "source": [
        "We now visualize the movement of the trained agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTDCjGia3-Jd"
      },
      "outputs": [],
      "source": [
        "# 6-1. Make the gif file of the trained agent's movement.\n",
        "movement_gif(\"lake_trained.gif\", q_table)\n",
        "\n",
        "# 6-2. Visualize the gif file.\n",
        "IPython.display.Image(filename=\"lake_trained.gif\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
